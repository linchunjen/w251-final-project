# -*- coding: utf-8 -*-
"""Kaggle_Interpret_Sign_Language_with_Deep_Learning_with_29_classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jIrtIh1pZtxb6so2x6I7BoNClgDSMRnL

### Code reference : [Kaggle kernel](https://www.kaggle.com/paultimothymooney/interpret-sign-language-with-deep-learning)
"""

#!pip install tensorflow==1.14.0



# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import tensorflow as tf
print('TensorFlow version: {}'.format(tf.__version__))

import sys

print(sys.version_info.major)
# Confirm that we're using Python 3
assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'

# Commented out IPython magic to ensure Python compatibility.
import keras
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization
from keras.utils.np_utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from keras import models, layers, optimizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import class_weight
from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop
from keras.models import Sequential, model_from_json
from keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization
#from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras import backend as K
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.applications.inception_v3 import InceptionV3
import os
from glob import glob
import matplotlib.pyplot as plt
import random
import cv2
import pandas as pd
import numpy as np
import matplotlib.gridspec as gridspec
import seaborn as sns
import zlib
import itertools
import sklearn
import itertools
import scipy
import skimage
from skimage.transform import resize
import csv
from tqdm import tqdm
from sklearn import model_selection
from sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
#from keras.applications.mobilenet import MobileNet
#from sklearn.metrics import roc_auc_score
#from sklearn.metrics import roc_curve
#from sklearn.metrics import auc
#import warnings
#warnings.filterwarnings("ignore")
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/My Drive/Colab Notebooks/asl-alphabet.zip'

# helper function for display the represent images
def plot_three_samples(letter):
    print("Samples images for letter " + letter)
    base_path = '/content/asl_alphabet_train/asl_alphabet_train/' + letter
    # img_path = base_path + letter #+ '/**'
    # path_contents = os.glob(img_path)
    imgs = np.random.choice(os.listdir(base_path), size = 3)
    
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(base_path + "/" +imgs[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(base_path + "/" +imgs[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(base_path + "/" +imgs[2]))
    return

plot_three_samples('E')

# helper function to get image size
def get_image_size(letter):
  from PIL import Image
  import numpy as np
  import os.path
  base_path = '/content/asl_alphabet_train/asl_alphabet_train/' + letter
  print(f"The base path is {base_path}")
  img = np.random.choice(os.listdir(base_path), size = 1)[0]
  print(f"The type of object returned is {type(img)}")
  print(img)
  image = Image.open(os.path.join(base_path, img))
  #image = Image.fromarray(np.uint8(img))
  print(image.size)

get_image_size('A')

"""##Data Preparation"""

imageSize=50
train_dir = '/content/asl_alphabet_train/asl_alphabet_train/'
test_dir =  '/content/asl_alphabet_test/asl_alphabet_test/'

# NEED TO UNCOMMENT RUN THIS CELL ONLY IF 
#X_train, X_test, y_train, y_test, y_trainHot and y_testHot have to generated again

from tqdm import tqdm
def get_data(folder):
    """
    Load the data and labels from the given folder.
    """
    X = []
    y = []
    for folderName in os.listdir(folder):
        if not folderName.startswith('.'):
            if folderName in ['A']:
                label = 0
            elif folderName in ['B']:
                label = 1
            elif folderName in ['C']:
                label = 2
            elif folderName in ['D']:
                label = 3
            elif folderName in ['E']:
                label = 4
            elif folderName in ['F']:
                label = 5
            elif folderName in ['G']:
                label = 6
            elif folderName in ['H']:
                label = 7
            elif folderName in ['I']:
                label = 8
            elif folderName in ['J']:
                label = 9
            elif folderName in ['K']:
                label = 10
            elif folderName in ['L']:
                label = 11
            elif folderName in ['M']:
                label = 12
            elif folderName in ['N']:
                label = 13
            elif folderName in ['O']:
                label = 14
            elif folderName in ['P']:
                label = 15
            elif folderName in ['Q']:
                label = 16
            elif folderName in ['R']:
                label = 17
            elif folderName in ['S']:
                label = 18
            elif folderName in ['T']:
                label = 19
            elif folderName in ['U']:
                label = 20
            elif folderName in ['V']:
                label = 21
            elif folderName in ['W']:
                label = 22
            elif folderName in ['X']:
                label = 23
            elif folderName in ['Y']:
                label = 24
            elif folderName in ['Z']:
                label = 25
            elif folderName in ['del']:
                label = 26
            elif folderName in ['nothing']:
                label = 27
            elif folderName in ['space']:
                label = 28           
            else:
                label = 27
            for image_filename in tqdm(os.listdir(folder + folderName)):
                img_file = cv2.imread(folder + folderName + '/' + image_filename)
                if img_file is not None:
                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))
                    img_arr = np.asarray(img_file)
                    X.append(img_arr)
                    y.append(label)
    X = np.asarray(X)
    y = np.asarray(y)
    return X,y
X_train, y_train = get_data(train_dir) 
#X_test, y_test= get_data(test_dir) # Too few images

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2) 

# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])
from keras.utils.np_utils import to_categorical
y_trainHot = to_categorical(y_train, num_classes = 29)
y_testHot = to_categorical(y_test, num_classes = 29)

## Persisting X_train, X_test, y_train, y_test and y_trainHot and y_testHot to Google drive to save some time
# save numpy array as npz file
from numpy import asarray
from numpy import savez_compressed

items = [("X_train",X_train),
         ("X_test",X_test),
         ("y_train",y_train),
         ("y_test",y_test),
         ("y_trainHot",y_trainHot),
         ("y_testHot",y_testHot)]

path = '/content/drive/My Drive/Berkeley_MIDS/w251/final_project/data/processed/29_labels/'
savez_compressed(path + "X_train" + ".npz", X_train=X_train)
savez_compressed(path + "X_test" + ".npz", X_test=X_test)
savez_compressed(path + "y_train" + ".npz", y_train=y_train)
savez_compressed(path + "y_test" + ".npz", y_test=y_test)
savez_compressed(path + "y_trainHot" + ".npz", y_trainHot=y_trainHot)
savez_compressed(path + "y_testHot" + ".npz", y_testHot=y_testHot)

path = '/content/drive/My Drive/Berkeley_MIDS/w251/final_project/data/processed/29_labels/'

from datetime import datetime

# datetime object containing current date and time
now = datetime.now()
 
print("now =", now)

# To load the data from Google Drive into X_train, X_test, y_train, y_test and y_trainHot and y_testHot
from datetime import datetime

# datetime object containing current date and time
now = datetime.now()

print(f"start of load = {now}")

# load numpy array from npz file
from numpy import load
# load dict of arrays

X_train_data= load(path + "X_train" + ".npz")
X_train = X_train_data['X_train']
X_test_data = load(path + "X_test" + ".npz")
X_test = X_test_data['X_test']
y_train_data = load(path + "y_train" + ".npz")
y_train =  y_train_data['y_train']
y_test_data = load(path + "y_test" + ".npz")
y_test = y_test_data['y_test']
y_trainHot_data = load(path + "y_trainHot" + ".npz")
y_trainHot = y_trainHot_data['y_trainHot']
y_testHot_data = load(path + "y_testHot" + ".npz")
y_testHot = y_testHot_data['y_testHot']

now = datetime.now()
print(f"end of load = {now}")

# def get_type_shape_info(an_array):
#   print(f"type={type(an_array)} size={an_array.shape}")

# for an_array in [X_train, y_trainHot, X_test, y_testHot]:
#   get_type_shape_info(an_array)

# print(len(X_train))

# Shuffle data to permit further subsampling
from sklearn.utils import shuffle
X_train, y_trainHot = shuffle(X_train, y_trainHot, random_state=13)
X_test, y_testHot = shuffle(X_test, y_testHot, random_state=13)
X_train = X_train[:30000]
X_test = X_test[:30000]
y_trainHot = y_trainHot[:30000]
y_testHot = y_testHot[:30000]

"""Step 3: Vizualize Data

The min/max pixel values are already scaled between 0 and 1
"""

def plotHistogram(a):
    """
    Plot histogram of RGB Pixel Intensities
    """
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.imshow(a)
    plt.axis('off')
    histo = plt.subplot(1,2,2)
    histo.set_ylabel('Count')
    histo.set_xlabel('Pixel Intensity')
    n_bins = 30
    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);
    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);
    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);
plotHistogram(X_train[5])

"""3 images from category "A""""

multipleImages = glob('/content/asl_alphabet_train/asl_alphabet_train/A/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2]))
    #;
plotThreeImages(multipleImages)

"""3 images from category "B""""

multipleImages = glob('/content/asl_alphabet_train/asl_alphabet_train/B/**')
def plotThreeImages(images):
    r = random.sample(images, 3)
    plt.figure(figsize=(16,16))
    plt.subplot(131)
    plt.imshow(cv2.imread(r[0]))
    plt.subplot(132)
    plt.imshow(cv2.imread(r[1]))
    plt.subplot(133)
    plt.imshow(cv2.imread(r[2])) 
plotThreeImages(multipleImages)

"""20 images from category "A""""

print("A")
multipleImages = glob('/content/asl_alphabet_train/asl_alphabet_train/A/**')
i_ = 0
plt.rcParams['figure.figsize'] = (10.0, 10.0)
plt.subplots_adjust(wspace=0, hspace=0)
for l in multipleImages[:25]:
    im = cv2.imread(l)
    im = cv2.resize(im, (128, 128)) 
    plt.subplot(5, 5, i_+1) #.set_title(l)
    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')
    i_ += 1

"""20 images from category "B""""

print("B")
multipleImages = glob('/content/asl_alphabet_train/asl_alphabet_train/B/**')
i_ = 0
plt.rcParams['figure.figsize'] = (10.0, 10.0)
plt.subplots_adjust(wspace=0, hspace=0)
for l in multipleImages[:25]:
    im = cv2.imread(l)
    im = cv2.resize(im, (128, 128)) 
    plt.subplot(5, 5, i_+1) #.set_title(l)
    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')
    i_ += 1

map_characters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z', 26: 'del', 27: 'nothing', 28: 'space'}
dict_characters=map_characters
import seaborn as sns
df = pd.DataFrame()
df["labels"]=y_train
lab = df['labels']
dist = lab.value_counts()
sns.countplot(lab)
print(dict_characters)

"""Step 4: Define Helper Functions"""

# Helper Functions  Learning Curves and Confusion Matrix

from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

class MetricsCheckpoint(Callback):
    """Callback that saves metrics after each epoch"""
    def __init__(self, savepath):
        super(MetricsCheckpoint, self).__init__()
        self.savepath = savepath
        self.history = {}
    def on_epoch_end(self, epoch, logs=None):
        for k, v in logs.items():
            self.history.setdefault(k, []).append(v)
        np.save(self.savepath, self.history)

def plotKerasLearningCurve():
    plt.figure(figsize=(10,5))
    metrics = np.load('logs.npy',allow_pickle=True)[()]
    filt = ['acc'] # try to add 'loss' to see the loss learning curve
    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):
        l = np.array(metrics[k])
        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')
        x = np.argmin(l) if 'loss' in k else np.argmax(l)
        y = l[x]
        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')
        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   
    plt.legend(loc=4)
    plt.axis([0, None, None, None]);
    plt.grid()
    plt.xlabel('Number of epochs')
    plt.ylabel('Accuracy')

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize = (8,8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def plot_learning_curve(history):
    plt.figure(figsize=(8,8))
    plt.subplot(1,2,1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig('./accuracy_curve.png')
    plt.subplot(1,2,2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig('./loss_curve.png')

"""Step 5: Evaluate Classification Models

Transfer learning w/ VGG16 Convolutional Network
"""

map_characters1 = map_characters
class_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
weight_path1 = '/content/drive/My Drive/Colab Notebooks/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'
weight_path2 = '/content/drive/My Drive/Colab Notebooks/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'
pretrained_model_1 = VGG16(weights = weight_path1, include_top=False, input_shape=(imageSize, imageSize, 3))
#pretrained_model_2 = InceptionV3(weights = weight_path2, include_top=False, input_shape=(imageSize, imageSize, 3))
optimizer1 = keras.optimizers.Adam()
optimizer2 = keras.optimizers.RMSprop(lr=0.0001)
def pretrainedNetwork(xtrain,ytrain,xtest,ytest,pretrainedmodel,pretrainedweights,classweight,numclasses,numepochs,optimizer,labels):
    base_model = pretrained_model_1 # Topless
    # Add top layer
    x = base_model.output
    x = Flatten()(x)
    predictions = Dense(numclasses, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    # Train top layer
    for layer in base_model.layers:
        layer.trainable = False
    model.compile(loss='categorical_crossentropy', 
                  optimizer=optimizer, 
                  metrics=['accuracy'])
    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]
    model.summary()
    print(f"epochs={numepochs} class_weight={classweight}")
    print(f"xtrain size={xtrain.size}")
    # Fit model
    trained_model = model.fit(xtrain,ytrain, epochs=numepochs, class_weight=classweight, validation_data=(xtest,ytest), verbose=1,callbacks = [MetricsCheckpoint('logs')])
    # Evaluate model
    score = model.evaluate(xtest,ytest, verbose=0)
    print('\nKeras CNN - accuracy:', score[1], '\n')
    return model, trained_model, score

model_spec, trained_model, score = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,29,10,optimizer1,map_characters1)

def score_and_plot(model, trained_model, xtest,ytest,labels):
  y_pred = model.predict(xtest)
  print('\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(y_pred, axis=1), target_names=list(labels.values())), sep='') 
  Y_pred_classes = np.argmax(y_pred,axis = 1) 
  Y_true = np.argmax(ytest,axis = 1) 
  confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
  plotKerasLearningCurve()
  plt.show()
  plot_learning_curve(trained_model)
  plt.show()
  plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))
  plt.show()

trained_model.history

score_and_plot(model_spec, trained_model, X_test, y_testHot, map_characters)

"""# Save Model"""

def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    """
    Freezes the state of a session into a pruned computation graph.

    Creates a new computation graph where variable nodes are replaced by
    constants taking their current value in the session. The new graph will be
    pruned so subgraphs that are not necessary to compute the requested
    outputs are removed.
    @param session The TensorFlow session to be frozen.
    @param keep_var_names A list of variable names that should not be frozen,
                          or None to freeze all the variables in the graph.
    @param output_names Names of the relevant graph outputs.
    @param clear_devices Remove the device directives from the graph for better portability.
    @return The frozen graph definition.
    """
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.global_variables()]
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = ""
        frozen_graph = tf.graph_util.convert_variables_to_constants(
            session, input_graph_def, output_names, freeze_var_names)
        return frozen_graph

from tensorflow.python.tools import freeze_graph

def save(session, directory, filename):
  if not os.path.exists(directory):
    os.makedirs(directory)
  
  filepath = os.path.join(directory, filename + '.ckpt')
  tf.train.Saver().save(session, filepath)
  return filepath

def save_as_pb(session, directory, filename):
  if not os.path.exists(directory):
    os.makedirs(directory)
    
  # Save check point for graph frozen later
  ckpt_filepath = save(session,directory=directory, filename=filename)
  pbtxt_filename = filename + '.pbtxt'
  pbtxt_filepath = os.path.join(directory, pbtxt_filename)
  pb_filepath = os.path.join(directory, filename + '.pb')
  # This will only save the graph but the variables will not be saved.
  # You have to freeze your model first.
  tf.train.write_graph(
      graph_or_graph_def=session.graph_def,
      logdir=directory,
      name=pbtxt_filename,
      as_text=True
  )
  
  # Freeze graph
  # Method 1
  # freeze_graph.freeze_graph(
  #     input_graph=pbtxt_filepath,
  #     input_saver='',
  #     input_binary=False,
  #     input_checkpoint=ckpt_filepath,
  #     output_node_names='cnn/output',
  #     restore_op_name='save/restore_all',
  #     filename_tensor_name='save/Const:0',
  #     output_graph=pb_filepath,
  #     clear_devices=True,
  #     initializer_nodes=''
  # )

  #Method 2
  frozen_graph = freeze_session(
      K.get_session(),
      output_names=[out.op.name for out in model.outputs]
  )
  tf.train.write_graph(
      frozen_graph,
      "/content/drive/My Drive/Colab Notebooks/models/Kaggle_PTMooney/29_labels/",
      "vgg16_model.pb",
      as_text=False
  )

  return pb_filepath, pbtxt_filepath, ckpt_filepath

from keras import backend as K
directory_path = "/content/drive/My Drive/Colab Notebooks/models/Kaggle_PTMooney/29_labels/"
filename = "vgg16_model.pb"
pb_filepath, pbtxt_filepath, ckpt_filepath = save_as_pb(
    K.get_session(),
    directory=directory_path,
    filename=filename)

"""# Load and Predict"""